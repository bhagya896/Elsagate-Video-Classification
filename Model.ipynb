{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "HOME_PATH = \"C:/Users/Bhagyashree/Desktop/project/kidsguard-dataset/video_splits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_directories = []\n",
    "for file in os.listdir(\"C:/Users/Bhagyashree/Desktop/project/kidsguard-dataset/videos/\"):\n",
    "    file = file.split('.')[0]\n",
    "    frame_directories.append(os.path.join(HOME_PATH, file)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def read_image(img_path):\n",
    "    if os.path.isfile(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        return np.asarray(img)\n",
    "    else:\n",
    "        return np.zeros((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-19 is donwloaded\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "layers = list(vgg19.features.children())\n",
    "layers.append(nn.AdaptiveMaxPool2d(1))\n",
    "modified_vgg19 = nn.Sequential(*layers)\n",
    "for p in modified_vgg19.parameters():\n",
    "    p.requires_grad = False\n",
    "modified_vgg19.eval()\n",
    "if use_cuda:\n",
    "    modified_vgg19.cuda()\n",
    "print(modified_vgg19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features are extracted from every frame\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "normalizer = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "def get_vgg_features_from_frame(frame_paths):\n",
    "    tensor_list = []\n",
    "    for frame_path in frame_paths:\n",
    "        frame = read_image(frame_path)\n",
    "        normalized_frame = normalizer(frame)\n",
    "        normalized_frame = normalized_frame.unsqueeze(0)\n",
    "        tensor_list.append(normalized_frame)\n",
    "    frame_tensors = Variable(torch.cat(tensor_list, 0))\n",
    "    if use_cuda:\n",
    "        frame_tensors = frame_tensors.cuda()\n",
    "    frame_features = modified_vgg19(frame_tensors)\n",
    "    frame_features = frame_features.view(frame_features.shape[0], frame_features.shape[1])\n",
    "    np_frame_features = frame_features.cpu().data.numpy()\n",
    "    start = np_frame_features.shape[0]\n",
    "    for i in range(start, 6):\n",
    "        np_frame_features = np.insert(np_frame_features, i, 0, axis=0)\n",
    "    return np_frame_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted features are saved into framefeatures.hdf5 file\n",
    "import h5py\n",
    "\n",
    "def save_checkpoint(frame_data, video_ids, path='C:/Users/Bhagyashree/Desktop/project/kidsguard-dataset/processed/aggregate_1_sec/frames_features.hdf5'):\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        os.makedirs(os.path.dirname(path))\n",
    "    with h5py.File(path, 'a', libver='latest') as f:\n",
    "        frame_data = np.array(frame_data)\n",
    "        video_ids = np.array(video_ids)\n",
    "        \n",
    "        try:\n",
    "            frame_dset = f['frames']\n",
    "            vids_dset = f['vids']\n",
    "        except KeyError:\n",
    "            frame_dset = f.create_dataset('frames', shape=(0, 6, 512), maxshape=(None, 6, 512), compression = 'gzip')\n",
    "            vids_dset = f.create_dataset('vids', shape=(0, ), maxshape=(None, ), compression = 'gzip', dtype=h5py.special_dtype(vlen=str))\n",
    "            f.swmr_mode = True\n",
    "\n",
    "        new_frame_shape = frame_data.shape[0]\n",
    "        new_vids_shape = video_ids.shape[0]\n",
    "        \n",
    "        frame_dset.resize(frame_dset.shape[0] + new_frame_shape, axis=0)\n",
    "        vids_dset.resize(vids_dset.shape[0] + new_vids_shape, axis=0)\n",
    "        \n",
    "        frame_dset[-new_frame_shape:] = frame_data\n",
    "        vids_dset[-new_vids_shape:] = video_ids\n",
    "        print(frame_dset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model is navigated to every frame that is extracted from the video so that VGG-19 can extract the features of the frames\n",
    "s=0\n",
    "for directory in frame_directories:\n",
    "    features = []\n",
    "    vids = []\n",
    "    frame_list=[]\n",
    "    for root, directories, files in os.walk(directory, topdown=False):\n",
    "        for name in files:\n",
    "            frame_list.append(os.path.join(root, name))\n",
    "        for name in directories:\n",
    "            os.path.join(root, name)\n",
    "\n",
    "    frame_files = natural_sort(frame_list)\n",
    "    frame_files_per_second = list(chunks(frame_files, 6))\n",
    "    ctr = 0\n",
    "    for frames_per_second in frame_files_per_second:\n",
    "        if len(frames_per_second) > 1:\n",
    "            frame_features = get_vgg_features_from_frame(frames_per_second)\n",
    "            features.append(frame_features)\n",
    "            vids.append(directory.split(os.sep)[-1])\n",
    "            ctr += 1\n",
    "        save_checkpoint(features, vids)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_PATH = 'C:/Users/Bhagyashree/Desktop/project/kidsguard-dataset/'\n",
    "\n",
    "ANNOTATION_PATH_SUFFIX = 'annotations/{0}.txt'\n",
    "DATASET_PATH = 'processed/annotated_data.hdf5'\n",
    "\n",
    "ANNOTATION_LABELS = {\n",
    "    'none': 0,\n",
    "    'violent' : 1,\n",
    "    'sexual': 2,\n",
    "    'both': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5(name, path=HOME_PATH + 'processed/aggregate_1_sec/frames_features.hdf5'):\n",
    "    f = h5py.File(path, 'r')\n",
    "    print(f)\n",
    "    return f[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = read_hdf5('vids')\n",
    "frames = read_hdf5('frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = vids[0]\n",
    "vid_details = []\n",
    "\n",
    "vid_dict = {}\n",
    "vid_dict['start_index'] = 0\n",
    "ctr = 0\n",
    "\n",
    "for i in range(0, vids.shape[0]):\n",
    "    vid = vids[i]\n",
    "    ctr += 1\n",
    "    if not prev == vid:\n",
    "        vid_dict['vid'] = prev\n",
    "        vid_dict['length'] = ctr\n",
    "        vid_details.append(vid_dict)\n",
    "        \n",
    "        vid_dict = {}\n",
    "        vid_dict['start_index'] = i\n",
    "        ctr = 0\n",
    "        prev = vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annotations of every frame are created and stored into Annotation.hdf5 file\n",
    "def save_checkpoint1(frame_data,annotations, video_ids):\n",
    "    path=HOME_PATH+DATASET_PATH\n",
    "    with h5py.File(path, 'a', libver='latest') as f:\n",
    "        frame_data = np.array(frame_data)\n",
    "        annotations = np.array(annotations)\n",
    "        video_ids = np.array(video_ids)\n",
    "        \n",
    "        try:\n",
    "            frame_dset = f['frames']\n",
    "            annotation_dset = f['annotations']\n",
    "            vids_dset = f['vids']\n",
    "        except KeyError:\n",
    "            frame_dset = f.create_dataset('frames', shape=(0, 6, 512), maxshape=(None, 6, 512), compression = 'gzip')\n",
    "            annotation_dset = f.create_dataset('annotations', shape=(0, ), maxshape=(None,), compression = 'gzip')\n",
    "            vids_dset = f.create_dataset('vids', shape=(0, ), maxshape=(None, ), compression = 'gzip', dtype=h5py.special_dtype(vlen=str))\n",
    "            f.swmr_mode = True\n",
    "\n",
    "        new_frame_shape = frame_data.shape[0]\n",
    "        new_annotation_shape = annotations.shape[0]\n",
    "        new_vids_shape = video_ids.shape[0]\n",
    "        \n",
    "        frame_dset.resize(frame_dset.shape[0] + new_frame_shape, axis=0)\n",
    "        annotation_dset.resize(annotation_dset.shape[0] + new_annotation_shape, axis=0)\n",
    "        vids_dset.resize(vids_dset.shape[0] + new_vids_shape, axis=0)\n",
    "        \n",
    "        frame_dset[-new_frame_shape:] = frame_data\n",
    "        annotation_dset[-new_annotation_shape:] = annotations\n",
    "        vids_dset[-new_vids_shape:] = video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations of every frame are created\n",
    "path1=\"C:/Users/Bhagyashree/Desktop/project/kidsguard-dataset/videos/Annotations/\"\n",
    "for file in os.listdir(path1):\n",
    "    \n",
    "    for detail in vid_details:\n",
    "        vid = detail['vid']\n",
    "        bad_annotation_ctr = 0\n",
    "        frame_index = 0\n",
    "        annotated_features = []\n",
    "        annotations = []\n",
    "        annotation_vids = []\n",
    "        file_path=path1+file\n",
    "        print(file_path)\n",
    "        with open(file_path) as f:\n",
    "            content = f.readlines()\n",
    "            content = [x.strip() for x in content]\n",
    "            for annotation in content:\n",
    "                m = re.search('[^: ]+$', annotation)\n",
    "                try:\n",
    "                    annotated_features.append(detail['start_index'])\n",
    "                    annotation_vids.append(vid)\n",
    "                except KeyError:\n",
    "                    bad_annotation_ctr += 1\n",
    "                    pass\n",
    "                frame_index += 1\n",
    "        save_checkpoint1(frames,annotations,annotation_vids)\n",
    "        assert len(annotated_features) + bad_annotation_ctr <= detail['length'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_DATASET = 'processed/aggregate_1_sec/frames_features.hdf5'\n",
    "\n",
    "SECONDS_PER_CLIP = 3\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "WRITE_DATASET = 'processed/aggregate_{0}_sec/frames_features.hdf5'.format(SECONDS_PER_CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=HOME_PATH+READ_DATASET\n",
    "def read_hdf5(name, path):\n",
    "    f = h5py.File(path, 'r')\n",
    "    return f[name][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = read_hdf5('frames',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_frame = np.reshape(frames, (int(frames.shape[0] / SECONDS_PER_CLIP), frames.shape[1] * SECONDS_PER_CLIP, frames.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(frames, path=HOME_PATH+WRITE_DATASET):\n",
    "    if not os.path.exists(os.path.dirname(HOME_PATH+WRITE_DATASET)):\n",
    "        os.makedirs(os.path.dirname(HOME_PATH+WRITE_DATASET))\n",
    "    with h5py.File(path, 'w') as f:\n",
    "        f.create_dataset('frames', data=frames, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(reshaped_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_DATASET1 = 'processed/annotated_data.hdf5'\n",
    "\n",
    "SECONDS_PER_CLIP = 3\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "WRITE_DATASET1 = 'processed/aggregate_{0}_sec/unbalanced_data.hdf5'.format(SECONDS_PER_CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5_1(name, path=HOME_PATH + READ_DATASET1):\n",
    "    f = h5py.File(path, 'r')\n",
    "    print(f)\n",
    "    return f[name][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = read_hdf5_1('frames')\n",
    "annotations = read_hdf5_1('annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_frames = np.reshape(frames, (int(frames.shape[0] / SECONDS_PER_CLIP), frames.shape[1] * SECONDS_PER_CLIP, frames.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_annotations = []\n",
    "for i in range(0, annotations.shape[0], SECONDS_PER_CLIP):\n",
    "    reshaped_annotations.append(np.bincount(annotations[i:i+SECONDS_PER_CLIP].astype(int)).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_annotations = np.array(reshaped_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_check_ctr = [0 for i in range(NUM_CLASSES)]\n",
    "for i in range(reshaped_annotations.shape[0]):\n",
    "    label_check_ctr[int(reshaped_annotations[i])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unbalanced_data.hdf5 file is created which has all the annotations \n",
    "def save_data1(frames, annotations, path=HOME_PATH+WRITE_DATASET1):\n",
    "    if not os.path.exists(os.path.dirname(HOME_PATH+WRITE_DATASET1)):\n",
    "        os.makedirs(os.path.dirname(HOME_PATH+WRITE_DATASET1))\n",
    "    with h5py.File(path, 'w') as f:\n",
    "        f.create_dataset('frames', data=frames, compression='gzip')\n",
    "        f.create_dataset('annotations', data=annotations, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data1(reshaped_frames, reshaped_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_DATASET_PATH = 'processed/aggregate_{0}_sec/unbalanced_data.hdf5'.format(SECONDS_PER_CLIP)\n",
    "WRITE_SAFE_DATASET_PATH = 'processed/aggregate_{0}_sec/safe_data.hdf5'.format(SECONDS_PER_CLIP)\n",
    "WRITE_EXPLICIT_DATASET_PATH = 'processed/aggregate_{0}_sec/explicit_data.hdf5'.format(SECONDS_PER_CLIP)\n",
    "\n",
    "label_ctr = [12313, 6795, 2268, 3244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5_2(name, path=HOME_PATH + READ_DATASET_PATH):\n",
    "    f = h5py.File(path, 'r')\n",
    "    print(f)\n",
    "    return f[name][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = read_hdf5_2('frames')\n",
    "annotations = read_hdf5_2('annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint2(frame_data, annotations, path):\n",
    "    with h5py.File(path, 'a', libver='latest') as f:\n",
    "        frame_data = np.array(frame_data)\n",
    "        annotations = np.array(annotations)\n",
    "        \n",
    "        try:\n",
    "            frame_dset = f['frames']\n",
    "            annotation_dset = f['annotations']\n",
    "        except KeyError:\n",
    "            frame_dset = f.create_dataset('frames', shape=(0, 6*SECONDS_PER_CLIP, 512), maxshape=(None, 6*SECONDS_PER_CLIP, 512), compression = 'gzip')\n",
    "            annotation_dset = f.create_dataset('annotations', shape=(0, ), maxshape=(None,), compression = 'gzip')\n",
    "            f.swmr_mode = True\n",
    "\n",
    "        new_frame_shape = frame_data.shape[0]\n",
    "        new_annotation_shape = annotations.shape[0]\n",
    "        \n",
    "        frame_dset.resize(frame_dset.shape[0] + new_frame_shape, axis=0)\n",
    "        annotation_dset.resize(annotation_dset.shape[0] + new_annotation_shape, axis=0)\n",
    "        \n",
    "        frame_dset[-new_frame_shape:] = frame_data\n",
    "        annotation_dset[-new_annotation_shape:] = annotations\n",
    "        print(frame_dset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A safe_data.hdf5 and explicit_data.hdf5 file is created which is used to store the data which is further passed to the models\n",
    "lvl_0_annotated_frames = []\n",
    "lvl_0_annotation_labels = []\n",
    "\n",
    "lvl_1_annotated_frames = []\n",
    "lvl_1_annotation_labels = []\n",
    "\n",
    "print(np.sum(label_ctr))\n",
    "save_every = 1000\n",
    "ctr = 0\n",
    "\n",
    "rand_indices = np.random.permutation(frames.shape[0])\n",
    "\n",
    "for rand_idx in rand_indices:\n",
    "    if not np.any(label_ctr):\n",
    "        break\n",
    "    print(\"rand index {0}\",rand_idx)\n",
    "    label = int(annotations[rand_idx])\n",
    "    if label_ctr[label] > 0:     \n",
    "        ctr += 1\n",
    "        if label == 0:\n",
    "            lvl_0_annotated_frames.append(frames[rand_idx])\n",
    "            lvl_0_annotation_labels.append(annotations[rand_idx])\n",
    "        else:\n",
    "            lvl_0_annotated_frames.append(frames[rand_idx])\n",
    "            lvl_0_annotation_labels.append(1)\n",
    "            \n",
    "            lvl_1_annotated_frames.append(frames[rand_idx])\n",
    "            lvl_1_annotation_labels.append(annotations[rand_idx]-1)\n",
    "            \n",
    "        if ctr % save_every == 0:\n",
    "            save_checkpoint2(lvl_0_annotated_frames, lvl_0_annotation_labels, HOME_PATH+WRITE_SAFE_DATASET_PATH)\n",
    "            save_checkpoint2(lvl_1_annotated_frames, lvl_1_annotation_labels, HOME_PATH+WRITE_EXPLICIT_DATASET_PATH)\n",
    "            print(label_ctr)\n",
    "            lvl_0_annotated_frames = []\n",
    "            lvl_0_annotation_labels = []\n",
    "\n",
    "            lvl_1_annotated_frames = []\n",
    "            lvl_1_annotation_labels = []\n",
    "            \n",
    "save_checkpoint2(lvl_0_annotated_frames, lvl_0_annotation_labels, HOME_PATH+WRITE_SAFE_DATASET_PATH)\n",
    "save_checkpoint2(lvl_1_annotated_frames, lvl_1_annotation_labels, HOME_PATH+WRITE_EXPLICIT_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl0_annotations = read_hdf5('annotations', HOME_PATH+WRITE_SAFE_DATASET_PATH)\n",
    "lvl1_annotations = read_hdf5('annotations', HOME_PATH+WRITE_EXPLICIT_DATASET_PATH)\n",
    "\n",
    "label_check_ctr = [0, 0, 0, 0]\n",
    "lvl1_ctr = 0\n",
    "for i in range(lvl0_annotations.shape[0]):\n",
    "    lvl0_label = int(lvl0_annotations[i])\n",
    "    if lvl0_label == 0:\n",
    "        label_check_ctr[lvl0_label] += 1\n",
    "    else:\n",
    "        lvl1_label = int(lvl1_annotations[lvl1_ctr]) + 1\n",
    "        lvl1_ctr += 1\n",
    "        label_check_ctr[lvl1_label] += 1\n",
    "print(label_check_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_DATASET_PATH1 = 'processed/aggregate_{0}_sec/unbalanced_data.hdf5'.format(SECONDS_PER_CLIP)\n",
    "WRITE_DATASET_PATH1 = 'processed/aggregate_{0}_sec/balanced_data.hdf5'.format(SECONDS_PER_CLIP)\n",
    "label_ctr1 = [12313, 6795, 2268, 3244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5_3(name, path=HOME_PATH + READ_DATASET_PATH):\n",
    "    f = h5py.File(path, 'r+')\n",
    "    return f[name][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = read_hdf5_3('frames')\n",
    "annotations = read_hdf5_3('annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_check_ctr1 = [0, 0, 0, 0]\n",
    "for i in range(annotations.shape[0]):\n",
    "    label_check_ctr1[int(annotations[i])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced_data.hdf5 file is created which has all the annotations in the proper format\n",
    "def save_checkpoint3(frame_data, annotations):\n",
    "    path=HOME_PATH+WRITE_DATASET_PATH\n",
    "    with h5py.File(path, 'a', libver='latest') as f:\n",
    "        frame_data = np.array(frame_data)\n",
    "        annotations = np.array(annotations)\n",
    "        \n",
    "        try:\n",
    "            frame_dset = f['frames']\n",
    "            annotation_dset = f['annotations']\n",
    "        except KeyError:\n",
    "            frame_dset = f.create_dataset('frames', shape=(0, 6*SECONDS_PER_CLIP, 512), maxshape=(None, 6*SECONDS_PER_CLIP, 512), compression = 'gzip')\n",
    "            annotation_dset = f.create_dataset('annotations', shape=(0, ), maxshape=(None,), compression = 'gzip')\n",
    "            f.swmr_mode = True\n",
    "\n",
    "        new_frame_shape = frame_data.shape[0]\n",
    "        new_annotation_shape = annotations.shape[0]\n",
    "        \n",
    "        frame_dset.resize(frame_dset.shape[0] + new_frame_shape, axis=0)\n",
    "        annotation_dset.resize(annotation_dset.shape[0] + new_annotation_shape, axis=0)\n",
    "        \n",
    "        frame_dset[-new_frame_shape:] = frame_data\n",
    "        annotation_dset[-new_annotation_shape:] = annotations\n",
    "        print(frame_dset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_frames = []\n",
    "annotation_labels = []\n",
    "\n",
    "print(np.sum(label_ctr1))\n",
    "save_every = 2000\n",
    "ctr = 0\n",
    "rand_indices = np.random.permutation(frames.shape[0])\n",
    "for rand_idx in rand_indices:\n",
    "    if not np.any(label_ctr):\n",
    "        break\n",
    "    label = int(annotations[rand_idx])\n",
    "    if label_ctr1[label] > 0:     \n",
    "        ctr += 1\n",
    "        annotated_frames.append(frames[rand_idx])\n",
    "        annotation_labels.append(annotations[rand_idx])\n",
    "        label_ctr[label] = label_ctr[label] - 1\n",
    "        if ctr % save_every == 0:\n",
    "            save_checkpoint3(annotated_frames, annotation_labels)\n",
    "            print(label_ctr)\n",
    "            annotated_frames = []\n",
    "            annotation_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECONDS_PER_CLIP = 3\n",
    "NUM_CLASSES = 3\n",
    "PROMINENT_TEST_CLASS = 2\n",
    "READ_DATASET_FILE2 = 'explicit_data'\n",
    "READ_DATASET_PATH2 = 'processed/aggregate_{0}_sec/{1}.hdf5'.format(SECONDS_PER_CLIP, READ_DATASET_FILE2)\n",
    "WRITE_TRAIN_DATASET_PATH2 = 'processed/aggregate_{0}_sec/train_balanced_data.hdf5'.format(SECONDS_PER_CLIP, READ_DATASET_FILE2)\n",
    "WRITE_TEST_DATASET_PATH2 = 'processed/aggregate_{0}_sec/test_balanced_data.hdf5'.format(SECONDS_PER_CLIP, READ_DATASET_FILE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5_4(name, path=HOME_PATH+READ_DATASET_PATH2):\n",
    "    f = h5py.File(path, 'r')\n",
    "    return f[name][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = read_hdf5_4('frames')\n",
    "annotations = read_hdf5_4('annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_splits(frames, annotations, path):\n",
    "    with h5py.File(path, 'w') as f:\n",
    "        f.create_dataset('frames', data=frames, compression='gzip')\n",
    "        f.create_dataset('annotations', data=annotations, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_train, frames_test, annnotations_train, annotations_test = train_test_split(frames, annotations, test_size=0.2, random_state=42, shuffle=True, stratify=annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [0 for i in range(NUM_CLASSES)]\n",
    "for i in annotations.astype(int):\n",
    "    label[i] += 1 \n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = [0 for i in range(NUM_CLASSES)]\n",
    "for i in annnotations_train.astype(int):\n",
    "    label_train[i] += 1 \n",
    "label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = [0 for i in range(NUM_CLASSES)]\n",
    "for i in annotations_test.astype(int):\n",
    "    label_test[i] += 1 \n",
    "label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_balanced_data.hdf5 and test_balanced_data.hdf5 file is created\n",
    "save_splits(frames_train, annnotations_train, HOME_PATH+WRITE_TRAIN_DATASET_PATH2)\n",
    "save_splits(frames_test, annotations_test, HOME_PATH+WRITE_TEST_DATASET_PATH2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
